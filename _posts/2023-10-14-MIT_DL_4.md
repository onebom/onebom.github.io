---
title: 4.Numerical Computation [Deep Learning-MITpress 번역본]
author: onebom
date: 2023-10-14 00:00:00 +0800
categories: [DL]
tags: []
toc: true
math: true
---

ML알고리즘은 일반적으로 많은 computation을 필요로합니다.   
이는 일반적으로 정확한 해에 대한 기호식을 제공하기 위해 공식을 분석적으로 도출하는 것이 아니라 반복적인 과정을 통해 해의 추정치를 갱신하는 방법으로 수학적 문제를 해결하는 알고리즘을 말합니다. 일반적인 연산으로는 최적화(함수를 최소화 또는 최대화하는 인수의 값을 찾는것)와 연립 방정식을 푸는 것이 있습니다.    
디지털 컴퓨터에서 수학적 함수를 평가하는 것만으로도 함수에 실수가 포함된 경우에는 어려울 수 있다. 실수는 유한한 용량의 메모리를 사용하여 정확하게 표현할 수 없기때문입니다.

## 1. Overflow and Underflow
디지털 컴퓨터에서 countinous math를 수행할 때 근본적인 어려움은 유한한 수의 비트 패턴으로 무한히 많은 실수를 표현해야 한다는 것입니다. 이는 거의 모든 실수의 경우 컴퓨터에서 그 수를 나타낼 때 근사 오차가 발생한다는 것을 의미합니다.     
- 많은 경우 이는 단순한 반올림 오차에 불과합니다. 
- 반올림 오차는 특히 많은 연산에 걸쳐 복합적으로 작용할 때 문제가 되며 이론적으로 작동하는 알고리즘이 반올림 오차의 누적을 최소화하도록 설계되지 않은 경우 실제로 실패할 수 있습니다.

특히 치명적인 반올림 오차의 한 형태는 **underflow**입니다.    
언더플로우는 0에 가까운 숫자가 0으로 반올림될 때 발생합니다. 
- 많은 함수들은 인수가 작은 양수가 아니라 0일 때 질적으로 다르게 동작합니다. 
- 예를 들어, 우리는 보통 0으로 나눗셈하는 것을 피하기를 원합니다(어떤 소프트웨어 환경에서는 예외가 발생하면 자리 표시자가 아닌 값으로 결과를 반환하고), 다른 소프트웨어 환경에서는 0으로 로그를 취합니다(이것은 보통 - ∞로 취급되며, 이후 많은 산술 연산에 사용되면 숫자가 아닙니다). 

또 다른 매우 손상적인 형태의 수치 오차는 **overflow**입니다.    
오버플로는 크기가 큰 수를 ∞ 또는 -∞로 근사할 때 발생합니다.  
- 산술은 일반적으로 이러한 무한 값을 숫자가 아닌 값으로 변경합니다. 


언더플로우 및 오버플로우에 대해 안정화되어야 하는 함수의 한 예는 **softmax** 함수입니다. 
- softmax 함수는 종종 다중 서울 분포와 관련된 확률을 예측하기 위해 사용됩니다. softmax 함수는 다음과 같이 정의됩니다

$$softmax(x_i) = {exp(x_i) \over \sum_{j=1}^n exp(x_j)}$$

모든 xi가 어떤 상수 c와 같을 때 무슨 일이 일어나는지 생각해보세요.   
분석적으로 모든 출력이 1/n과 같아야 한다는 것을 알 수 있습니다. 
- 수치적으로 c가 큰 크기일 때는 이런 일이 일어나지 않을 수 있습니다. 
- c가 매우 음수이면 exp(c)는 밑으로 흐릅니다.    
  이것은 소프트맥스의 분모가 0이 된다는 것을 의미하므로 최종 결과는 정의되지 않습니다.
- c가 매우 크고 양수이면 exp(c)가 넘쳐서 식 전체가 정의되지 않습니다. 

이 두 가지 어려움은 대신 $$z = x - max_{i}x_i$$인 $$softmax(z)$$를 계산하면 해결할 수 있습니다. 단순 대수학은 입력 벡터에서 스칼라를 더하거나 뺄 때 소프트맥스 함수의 값이 분석적으로 변하지 않음을 보여줍니다. 
- $$max_{i}x_i$$ 빼면 exp에 대한 가장 큰 인수가 0이되어 오버플로우의 가능성이 배제됩니다. 
- 마찬가지로, 분모의 적어도 하나의 항은 1의 값을 갖는데, 이것은 분모에서 0으로 나눗셈에 이르게 하는 언더플로우의 가능성을 배제합니다. 

여전히 한 가지 작은 문제가 있습니다. 분자 내 언더플로우는 여전히 식을 전체적으로 0으로 평가하게 할 수 있습니다. 이것은 softmax 서브루틴을 먼저 실행한 후 결과를 로그 함수에 전달하여 로그 softmax(x)를 구현할 경우 - ∞를 잘못 얻을 수 있음을 의미합니다. 대신에 로그 softmax를 수치적으로 안정적인 방법으로 계산하는 별도의 함수를 구현해야 합니다. softmax 함수를 안정화할 때와 같은 방법으로 로그 softmax 함수를 안정화할 수 있습니다. 

우리는 대부분 이 책에서 설명한 다양한 알고리즘을 구현할 때 관련된 모든 수치적 고려 사항을 명시적으로 상술하지는 않습니다. 하위 수준 라이브러리 개발자는 딥러닝 알고리즘을 구현할 때 수치적 문제를 염두에 두어야 합니다. 이 책의 대부분의 독자는 단순히 안정적인 구현을 제공하는 하위 수준 라이브러리에 의존할 수 있습니다. 경우에 따라 새로운 알고리즘을 구현하고 새로운 구현을 자동으로 안정화하는 것이 가능합니다. Theano(Bergstra et al., 2010; Bastien et al., 2012)는 딥러닝 맥락에서 발생하는 많은 일반적인 수치적으로 불안정한 표현을 자동으로 검출하고 안정화하는 소프트웨어 패키지의 한 예입니다.


## 2.  Poor Conditioning
Conditioning는 함수가 입력의 작은 변화에 대해 얼마나 빠르게 변화하는지를 말합니다. 입력이 약간 동요할 때 빠르게 변화하는 함수는 입력의 반올림 오류가 출력의 큰 변화를 초래할 수 있기 때문에 과학적 계산에 문제가 될 수 있습니다.
함수 f(x) = A-1x 를 생각해봅시다. A ∈ R_nxn 가 고유값 분해를 가질 때, 그 조건 번호는
$$\max_{i,j} \left|{\lambda_i \over \lambda_j }\right|$$

이것은 가장 큰 고유값과 가장 작은 고유값의 크기의 비율입니다.    
이 숫자가 크면 행렬의 반전은 입력의 오류에 특히 민감합니다.   
- 이 민감도는 행렬 자체의 고유한 성질이지 행렬 반전 시 반올림 오차의 결과가 아닙니다
조건이 나쁜 행렬은 실제 행렬을 역으로 곱할 때 기존의 오차를 증폭합니다. 실제로는 반전 과정 자체에서 수치 오차로 인해 오차가 더욱 증폭됩니다.

## 3. Gradient-Based Optimization
대부분의 딥 러닝 알고리즘은 일종의 최적화를 수반합니다. 최적화는 x를 변경함으로써 일부 함수 f(x)를 최소화하거나 최대화하는 작업을 말합니다. 우리는 보통 f(x)를 최소화한다는 관점에서 대부분의 최적화 문제를 표현합니다. 최대화는 -f(x)를 최소화함으로써 최소화 알고리즘을 통해 달성될 수 있습니다.   

최소화 또는 최대화하고자 하는 함수를 **objective function** 또는 **criterion**이라고 합니다. 최소화할 때 **cost function, loss function or error function**라고도 부를 수 있습니다. 이 책에서는 이 용어들을 서로 교환하여 사용하지만, 일부 기계 학습 간행물에서는 이 용어들에 특별한 의미를 부여합니다.
우리는 종종 위첨자 ∗로 함수를 최소화 또는 최대화하는 값을 나타냅니다.    
예를 들어 $$x^∗ = \argmin f(x)$$라고 말할 수 있습니다.

우리는 독자가 미적분학에 이미 익숙하다고 가정하지만, 미적분학 개념이 최적화와 어떻게 관련되는지에 대한 간단한 검토를 제공합니다.    
x와 y가 모두 실수인 함수 y = f(x)가 있다고 가정합시다. 이 함수의 도함수(derivative)는 $$f'(x)$$ 또는 $${dx \over dy}$$로 표시된다. 도함수 f'(x)는 점 x에서 f(x)의 기울기를 제공합니다.     
즉, 출력의 해당 변화를 얻기 위해 입력의 작은 변화를 조정하는 방법을 지정합니다: $$f(x+\epsilon) ≈ f(x) + \epsilon f'(x)$$ 

![figure1](/assets/img/posts/MIT_DL4/figure1.png)   

따라서 도함수는 함수를 최소화하는 데 유용한데, 이는 y를 작게 개선하기 위해 x를 어떻게 바꾸는지 알려주기 때문입니다. 
- 예를 들어, 우리는 $$f(x-\epsilon sign(f'(x)))$$가 충분히 작은 $$\epsilon $$일 때 f(x)보다 작다는 것을 알고 있습니다.
- 따라서 도함수의 반대 부호를 가진 x를 작은 걸음으로 이동시킴으로써 f(x)를 줄입니다.  
이 기술은 **gradient descent**라고 불립니다

![figure2](/assets/img/posts/MIT_DL4/figure2.png)   

- f'(x) = 0인 경우, 도함수는 어느 방향으로 이동할지에 대한 정보를 제공하지 않습니다. - f'(x) = 0인 점을 **critical points 또는 stationary points**이라고 합니다. **local minimum**은 f (x)가 이웃한 모든 점보다 낮은 점이므로, 더 이상 극솟값으로 f(x)를 감소시킬 수 없습니다. 
- **local maximum**은 f (x)가 이웃한 모든 점보다 높은 점이므로, infinitesimal steps를 수행하여 f (x)를 증가시킬 수는 없습니다. 
- 일부 임계점은 극대도 극솟값도 아닙니다. 이것을 **saddle points**이라고 합니다. 각

f(x)의 절대 최저값을 구하는 점은 **global minimum**입니다.    
- 함수의 전역 최소값이 하나이거나 여러 개일 수 있습니다. 
- 전역적으로 최적이 아닌 local minimum이 있을 수도 있습니다. 
딥 러닝의 맥락에서, 우리는 최적이 아닌 많은 local minimum과 매우 평평한 영역으로 둘러싸인 많은 saddle points을 가질 수 있는 함수를 최적화합니다.    
특히 함수에 대한 입력이 다차원인 경우 이 모든 것이 최적화를 매우 어렵게 만듭니다.따라서 우리는 보통 매우 낮지만 반드시 형식적인 의미에서 최소는 아닌 f 값을 찾는 것으로 만족합니다.

우리는 종종 여러 입력이 있는 함수를 최소화합니다: $$f : \mathbb{R}_n → \mathbb{R}$$.   
- "minimization"의 컨샙이 말이되려면 하나의 (scalar) 출력만이 있어야 합니다.    

다수의 입력을 가지는 함수에 대하여, 우리는 **partial derivatives**(편미분)의 개념을 사용해야 합니다. 편미분 $${ \partial \over \partial x_i}f(x)$$는 x지점에서 변수 x_i가 증가함에따라 f가 얼마나 변하는가를 측정합니다. **gradient**는 도함수가 벡터에 대해 존재하는 경웨 도함수의 개념을 일반화합니다: f의 기울기는 $${\nabla}_x f(x)$$로 표시되는 편미분을 포함하는 벡터입니다.
- 기울기의 element i는 x_i에 대한 편미분입니다.
- 다차원에서, critical points는 기울기가 0인 모든 element가 있는 곳을 가르킵니다.

unit vector인 u의 방향에서의 **directional derivative**(방향 도함수)은 함수 f의 기울기입니다. 즉, directional derivative는 $$\alpha=0$$라고 evaluated되는 $$\alpha$$에 대한 함수 $$f(x+\alpha u)$$의 도함수다. chain rule을 사용하면 $$\alpha=0$$일때, $${\partial \over \partial \alpha}f(x+\alpha u)$$는 $$u^T {\nabla}_x f(x)$$로 쓸 수 있다.

f를 최소화하기 위해서, f가 가장 빠르게 감소하는 방향을 찾을것입니다. 우리는 directional derivative를 사용하여 찾을 수 있습니다;

$${\min}_{u,u^{T}u=1} u^T {\nabla}_x f(x)$$

$$= {\min}_{u,u^{T}u=1} \left| u \right|^2 \left| {\nabla}_x f(x) \right|^2 \cos \theta$$

- θ는 u와 gradient간의 각도입니다.
- ||u||2 = 1로 치환하고 u에 의존하지 않는 인자를 무시하면, $${\min}_u \cos \theta$$로 간단화할 수 있습니다.
- 이것은 u가 gradient와 반대방향을 가리킬 때 최소화됩니다.
- 즉 gradient는 오르막을 가리키고, negative gradient는 내리막을 가리킵니다.
우리는 negative gradient로 이동함으로써 f를 줄일 수 있습니다. 이 방법은 **steepest descent or gradient descent**라 알려져 있습니다.

**steepest descent**는 새로운 지점을 제시합니다;
$$x'=x - \epsilon {\nabla}_x f(x)$$
- \epsilon은 **learning rate**라 하며, step size를 결정하는 positive scalar입니다.
- 우리는 \epsilon을 다양한 방법으로 고를 수 있습니다.
  1. 일반적인 접근법은 epsilon을 작은 상수로 설정하는 것입니다. 때떄로 directional derivative vanish를 일으키는 step size에 대해 해결할 수 도 있습니다.
  2. 다른 접근법은 여러 epsilon 값에 대해 $$f(x - \epsilon {\nabla}_x f(x))$$ 를 계산하고 가장 작은 목적함수값을 선택하는 것입니다. 이 방식을 **line search**라고 합니다.

Steepest descent는 기울기의 모든 원소가 0일때(또는 실제로 0에 매우 가까울때) 수렴합니다. 어떤 경우에는, 반복 알고리즘을 실행하는 것을 피하고 x에 대한 $${\nabla}_x f(x)=0$$을 풀어내면 critical point를 바로 알아낼 수 있습니다.   

gradient descent는 continuous spaces에서의 최적화로 제한되지만, 더 나은 configurations을 향해 반복적으로 작은 움직임을 하는 일반적 개념의 discrete spaces으로 일반화할 수 있습니다.   
discrete 매개변수들의 목적함수를 상승시키는 것을 **hill climbing**이라고 합니다.

### 1) Beyond the Gradient: Jacobian and Hessian Matrices
때때로 우리는 입력과 출력이 모두 벡터인 함수의 모든 편미분을 찾을 필요가 있습니다. 이러한 모든 편미분을 포함하는 행렬을 **Jacobian matrix**이라고 합니다. 구체적으로, 만약 함수 f가 $$ \mathbb{R}^m \rightarrow \mathbb{R}^n $$이라면, f의 jacobian matrix $$J \in \mathbb{R}^{n \times m}$$은 $$J_{i,j}={ \partial \over \partial x_j}f(x)_i$$로 정의됩니다.

우린 때때로, derivative of a derivative에 관심이 있습니다. 이것은 second derivative로 알려져 있습니다. 예를 들어 함수 $$f: \mathbb{R}^n \rightarrow \mathbb{R}$$에 대하여, x_j에 대한 도함수의 x_i에 대한 도함수는 $$ {\partial^2 \over \partial x_i \partial x_j} f $$로 표시합니다.  
- 단일차원에서, 우리는 $${\partial^2 \over \partial x^2} f$$를 $$f(x)$$로 표시할수 있습니다.   
- 이계도함수는 우리가 입력을 변화시킬 때 1차 도함수가 어떻게 변화할 것인지 알려줍니다.  
이것은 gradient step이 기울기만을 기준으로 기대하는 만큼의 개선을 가져올지 알려주기 때문에 중요합니다. 우리는 2차 미분을 **curvature(곡률)**을 측정하는 것으로 생각할 수 있습니다.    
우리가 2차 함수를 가지고 있다고 가정합시다.    
1. 만약 그러한 함수가 zero의 2차 미분을 가지고 있다면, 곡률은 없습니다. 
  - 이것은 완벽하게 평평한 선이며 오직 gradient만을 사용하여 그 값을 예측할 수 있습니다. 
   - 기울기가 1이면 우리는 음의 구배를 따라 크기 epsilon의 단계를 만들 수 있고 cost function는 epsilon만큼 감소할 것입니다. 
2. 만약 2차 미분이 음수이면 함수는 아래로 휘어지기 때문에 실제로 cost function는 epsilon 이상 감소할 것입니다. 
3. 마지막으로 2차 미분이 양수이면 함수는 위로 휘어지기 때문에 cost function는 epsilon 미만으로 감소할 수 있습니다.

![figure3](/assets/img/posts/MIT_DL4/figure3.png)   
기울기로 예측된 cost function value과 true value 사이의 관계에 다양한 curvature 형태가 어떤 영향을 미치는지 보려면 그림 4.4를 참조하십시오.

우리의 함수가 여러 입력 차원을 가질 때, 많은 이계도함수가 있습니다. 이러한 도함수들은 **Hessian matrix**라고 불리는 행렬로 collected될수 있습니다. 헤센 행렬 $$H(f)(x)$$는 다음과 같이 정의됩니다;

$$H(f)(x)_{i,j} = { {\partial}^2 \over \partial x_i \partial x_j} f(x)$$

- 동등하게, Hessian은 gradient의 Jacobian입니다.

second partial derivatives가 연속적인 곳에서는, differential operators가 교환가능합니다. 즉 순서를 swap할수 있습니다;

$${ {\partial}^2 \over \partial x_i \partial x_j} f(x) = { {\partial}^2 \over \partial x_j \partial x_i} f(x)$$

- 이것은 $$H_{i,j} = H_{j,i}$$이므로 Hessian 행렬은 그러한 점에서 대칭임을 의미합니다. 

딥 러닝의 맥락에서 우리가 접하는 대부분의 함수는 거의 모든 곳에서 symmetric Hessian 행렬을 가지고 있습니다.     
헤센 행렬은 실수이고 대칭이기 때문에, 우리는 그것을 실수 eigenvalues 집합과 eigenvectors의 orthogonal basis로 분해할 수 있습니다.    
단위 벡터 d로 표현되는 특정 방향의 이계도함수는 $$d^{T}Hd$$로 표현됩니다. 
- d가 H의 고유벡터일 때, 그 direction의 이계도함수는 대응하는 eigenvalue에 의해 주어집니다. 
- d의 다른 방향에 대해, directional second derivative는 0과 1 사이의 가중치를 갖는 모든 eigenvalues의 가중 평균이며, d와 더 작은 각도를 갖는 고유벡터는 더 많은 가중치를 받습니다. 
- 최대 고윳값은 최대 이계도함수를 결정하고 최소 고윳값은 최소 이계도함수를 결정합니다.

(방향) 이계도함수는 gradient descent step이 얼마나 잘 수행될 것인지를 알려줍니다. 우리는 현재 점 x(0)을 중심으로 함수 f(x)에 대한 2차 테일러 급수 근사를 만들 수 있습니다:

$$f(x) = f(x^{(0)})+(x-x^{(0)})^{T}g+{1 \over 2}(x-x^{(0)})^{T}H(x-x^{(0)})$$

여기서 g는 gradient이고, H는 x^(0)에서의 Hessian 값입니다. 만약 우리가 epcilon의 학습률을 사용한다면, 새로운 점 x는 $$x^{(0)}-\epsilon g$$로 주어질 것입니다. 이를 근사에 대입하면 다음을 얻을 수 있습니다:

$$f(x^{(0)}-\epsilon g) = f(x^{(0)})-\epsilon g^{T}g+ { 1 \over 2} {\epsilon}^2 g^T Hg$$

- 여기에는 세 가지 항이 있습니다: 
  1. 함수의 original value, 
  2. 함수의 slop을 통한 expected improvement, 
  3. 함수의 curvature을 설명하기 위해 적용해야 하는 correction입니다. 
- 이 마지막 항이 너무 크면 gradient descent step은 실제로 오르막으로 이동할 수 있습니다. 
- $$g^{T}Hg$$가 0이거나 음수이면 테일러 급수 근사치는 증가하는 epcilon이 영원히 f를 감소시킬 것이라고 예측합니다. 
실제로 테일러 급수는 큰 epcilon에 대해 정확하지 않기 때문에 이 경우에는 의 더 많은 휴리스틱 선택에 의존해야 합니다. $$g^{T}Hg$$가 양수일 때, 함수의 테일러 급수 근사치를 감소시키는 최적 step size에 대한 해가 가장 많이 산출됩니다:

$$\epsilon^* = {g^Tg \over g^THg}$$

최악의 경우, g가 최대고윳값인 $${\lambda}_max$$에 해당하는 H의 고유벡터와 정렬될때, 이 최적의 step size는 1로 주어집니다. 우리가 최소화하는 함수가 2차 함수에 의해 잘 근사될 수 있는  $${\lambda}_max$$ 범위까지, Hessian의 고유값은 learning rate의 scale을 결정합니다.

second derivative는 critical point가 local maximum인지, minimum인지, saddle point인지를 결정하는 데 사용될 수 있습니다. 임계점에서 f'(x) = 0임을 기억하십시오. 
- 2차 도함수 f''(x) > 0일 때, 1차 도함수 f'(x)는 오른쪽으로 이동할 때 증가하고 왼쪽으로 이동할 때 감소합니다.
- 이것은 충분히 작은 epcilon에 대해 f'(x - e) < 0 및 f'(x + e) > 0을 의미합니다. 
- 즉, 우리가 오른쪽으로 이동할 때, 기울기는 오른쪽으로 오르막을 가리키기 시작하고, 왼쪽으로 이동할 때 기울기는 왼쪽으로 오르막을 가리키기 시작합니다. 
따라서 f'(x) = 0이고 f(x)''> 0일 때, 우리는 x가 최솟값이라고 결론 내릴 수 있습니다. 마찬가지로, f'(x) = 0이고 f''(x)< 0일 때, 우리는 x가 최댓값이라고 결론 내릴 수 있습니다.     
이것을 **second derivative test**라고 합니다. 
- 불행히도 f''(x) = 0일 때, 판정법은 확정적이지 않습니다. 
- 이 경우 x는 critical point일 수도 있고, 평평한 영역의 일부일 수도 있습니다.

다차원에서, 우리는 함수의 모든 이계도함수를 조사할 필요가 있습니다. Hessian 행렬의 고유분해를 사용하여, 우리는 second derivative test를 다차원으로 일반화할 수 있습니다. $\nabla_x f(x)=0$인 critical point에서,우리는 Hessian eigenvalues을 조사하여 critical point가 local maximum인지 local minimum인지, 또는 saddle point인지를 결정할 수 있습니다. 
1. Hessian 고유값이 positive definite(모든 고유값이 양)일 때, 그 점은 local minimum입니다. 
  - 이것은 directional second derivative가 어떤 방향으로든 양의 값이어야 한다는 것을 관찰하고, univariate second derivative test을 참조하면 알 수 있습니다. 
2. 마찬가지로, Hessian 고유값이 negative definite(모든 고유값이 음)일 때, 그 점은 local maximum입니다.
   - 다차원으로, 실제로 어떤 경우에 saddle points의 positive evidence를 발견할 수 있습니다. 
3. 적어도 하나의 고유값이 양의 값이고 적어도 하나의 고유값이 음의 값이면, 우리는 x가 f의 한 단면에서는 최댓값이지만 다른 단면에서는 최솟값이라는 것을 알 수 있습니다. 

![figure4](/assets/img/posts/MIT_DL4/figure4.png)     
그림 4.5를 예로 들어봅시다. univariate version과 마찬가지로 다차원 seconde derivative test는 결론이 나지 않을 수 있습니다. 
- test는 모든 0이 아닌 고유값이 같은 부호를 가지지만 적어도 하나의 고유값이 0일 때마다 결론이 나지 않습니다. 
- 이것은 univariate second derivative test가 0의 고유값에 해당하는 단면에서 결론이 나지 않기 때문입니다.

여러 차원에서 단일 지점의 각 방향에 대해 서로 다른 second derivative가 존재합니다. 이 지점의 Hessian의 condition number는 second derivative들이 서로 얼마나 다른지를 측정합니다.    
Hessian이 poor한 condition number를 가질때, gradient descent는 잘 수행되지 않습니다. 
- 이는 한 방향에서는 도함수가 급격히 증가하는 반면 다른 방향에서는 천천히 증가하기 때문입니다. 
- gradient descent는 도함수의 이러한 변화를 알지 못하기 때문에 도함수가 더 오랫동안 음수로 유지되는 방향을 우선적으로 탐색해야 함을 알 수 없습니다. 
- 또한 좋은 step size를 선택하는 것을 어렵게 합니다. 
  - step size는 1) minimum을 넘어가는 것을 방지하고, 2)양의 curvature가 강한 direction으로 오르막을 오르는 것을 방지할 수 있을 정도로 충분히 작아야 합니다. 
  - 이는 일반적으로 계단 크기가 너무 작아서, curvature이 낮은 다른 방향으로 큰 진전을 이룰 수 없음을 의미합니다.  
예를 들어 그림 4.6을 참조하십시오.

![figure5](/assets/img/posts/MIT_DL4/figure5.png)   

이 문제는 Hessian이 행렬의 정보를 이용하여 탐색을 유도함으로써 해결할 수 있습니다. 그렇게 하는 가장 간단한 방법은 Newton’s method로 알려져 있습니다.
- Newton’s method은 2차 테일러 급수 전개를 x(0)점 근처의 f(x)에 가깝게 사용하는 것에 기초를 두고 있습니다:

$$f(x)=f(x^{(0)})+(x-x^{(0)})^T \nabla_x f(x^{(0)})+ {1\over 2}(x-x^{(0)})^T H(f)(x^{(0)})(x-x^{(0)})$$

이 함수의 critical point를 풀면 다음을 얻을 수 있습니다:

$$ x*=x^{(0)} - H(f)(x^{(0)})^-1 \nabla_x f(x^{(0)}) $$

f가 positive definite quadratic function일때, Newton’s method은 함수의 최솟값으로 직접 점프하기 위해 위의 식를 한 번 적용하는 것으로 구성됩니다.    
f가 진정한 2차 함수가 아니고 positive definite quadratic function으로 locally하게 근사될 수 있을 때 뉴턴의 방법은 위의 식을 여러 번 적용하는 것으로 구성됩니다. 
- approximation를 반복적으로 업데이트하고 approximation의 최솟값으로 점프하는 것은 기울기 하강보다 임계점에 훨씬 빨리 도달할 수 있습니다.
- 이는 local minimum 근처에서 유용한 성질이지만 saddle point 근처에서 유해한 성질이 될 수 있습니다. 
- 8.2.3절에서 논의한 바와 같이 뉴턴의 방법은 critical point가 minimum일때(Hessian의 모든 고유값이 양수일때)만 적합하다. 반면, gradient descent는 gradient가 saddle point를 향하지 않는다면 saddle point로 가지않는다.

기울기 하강과 같이 기울기만을 사용하는 최적화 알고리즘을 **first-order optimization algorithms**이라고 합니다. 뉴턴의 방법처럼 헤센 행렬을 사용하기도 하는 최적화 알고리즘을 **second-order optimization algorithms**이라고 합니다.

이 책의 대부분의 맥락에서 사용되는 최적화 알고리즘은 매우 다양한 함수에 적용할 수 있지만 거의 guarantees가 없습니다. 딥러닝 알고리즘은 딥러닝에 사용되는 함수군이 상당히 복잡하기 때문에 guarantees가 부족한 경향이 있습니다. 다른 많은 분야에서 최적화에 대한 지배적인 접근 방식은 제한된 함수군에 대한 최적화 알고리즘을 설계하는 것입니다.

딥 러닝의 맥락에서, 우리는 때때로 Lipschitz continuous하거나 Lipschitz continuous derivatives를 갖는 함수로 제한함으로써 약간의 보장을 얻습니다.   
Lipschitz continuous function는 변화율이 Lipschitz constant L에 의해 제한되는 함수 f입니다:

$$\forall_x , \forall_y , \left| f(x)-f(y) \right| \le L {\left| x-y \right|}_2$$

이 속성은 기울기 강하와 같은 알고리즘에 의해 이루어지는 입력의 작은 변화가 출력의 작은 변화를 가져올 것이라는 가정을 정량화할 수 있기 때문에 유용합니다. Lipschitz continuity도 상당히 약한 제약 조건이며, 딥 러닝의 많은 최적화 문제는 상대적으로 약간의 수정으로 Lipschitz를 연속적으로 만들 수 있습니다.

전문화된 최적화의 가장 성공적인 분야는 아마도 **convex optimization**일 것입니다. convex optimization 알고리즘은 더 강력한 제한을 가함으로써 더 많은 보장을 제공할 수 있습니다. 
convex optimization 알고리즘은 Hessian이 positive semidefinite인 함수인 convex functions에만 적용될 수 있습니다. 
- 그러한 함수는 saddle point이 부족하고 모든 local minimum이 반드시 global minimum이기 때문에 잘 동작합니다. 
그러나 딥 러닝에서 대부분의 문제는 convex optimization의 측면에서 표현하기 어렵습니다. convex optimization는 일부 딥 러닝 알고리즘의 서브루틴으로만 사용됩니다.    
convex optimization 알고리즘의 분석에서 나온 아이디어는 딥 러닝 알고리즘의 수렴을 증명하는 데 유용할 수 있습니다. 그러나 일반적으로 딥 러닝의 맥락에서는 convex optimization의 중요성이 크게 감소합니다. convex optimization에 대한 자세한 내용은 Boyd and Vandenberghhe (2004) 또는 Rockafellar (1997)를 참조하십시오.

## 4. Constrained Optimization

때때로 우리는 x의 모든 가능한 값에 대하여 함수 f(x)를 극대화 또는 극소화하기를 원할 뿐만 아니라. 대신에 우리는 일부 집합 S에 있는 x의 값에 대하여 f(x)의 최댓값 또는 최솟값을 찾기를 원할 수 있습니다. 이를 **constrained optimization**라고 합니다. 집합 S 내에 있는 점 x를 constrained optimization 용어에서 **feasible points**라고 합니다.

우리는 종종 어떤 의미에서 작은 해를 찾고자 합니다. 그러한 상황에서 일반적인 접근법은 $$\left|| x \right|| \le 1$$과 같은 norm constraint을 가하는 것입니다.

constrained optimization을 위한 간단한 접근법은 단순히 constraint을 고려하여 gradient descent를 수정하는 것입니다. 만약 우리가 작은 constant step size epcilon를 사용한다면, 우리는 gradient descent steps를 만들고 그 결과를 S에 다시 투영할 수 있습니다. 
- 만약 우리가 line search을 사용한다면, 우리는 새로운 feasible x 점을 산출하는 step size epcilon 위에서만 검색할 수 있거나 우리는 선 위의 각 점을 다시 constraint region에 투영할 수 있습니다. 
- 가능한 경우, 이 방법은 단계를 수행하거나 line search을 시작하기 전에  feasible region의 접선 공간(tangent space)에 기울기를 투영함으로써 더 효율적으로 만들 수 있습니다

더 정교한 접근법은 원래의 제약된 최적화 문제의 해로 변환될 수 있는 다른 unconstrained optimization problem를 설계하는 것입니다.    
- 예를 들어, 정확히 단위 L2 표준을 갖도록 제약된 x ∈ R^2에 대해 f(x)를 최소화하려면, θ에 대한 $$ g(\theta) = f([\cos \theta, \sin \theta]^T)$$ 로 대체하여 최소화한 후 원래 문제의 해로 [cos θ, sin θ]를 반환할 수 있습니다. 
이 접근법은 창의력을 필요로 하며, 최적화 문제 간의 변환은 우리가 직면하는 각 경우에 대해 특별히 설계되어야 합니다.

**Karush-Kuhn-Tucker(KKT)** 접근법은 제한된 최적화에 대한 매우 일반적인 해결책을 제공합니다. KKT 접근법을 사용하여 **generalized Lagrangian or generalized Lagrange function**라는 새로운 함수를 소개합니다.

Lagrangian을 정의하기 위해서, 우리는 먼저 방정식과 부등식의 관점에서 S를 기술할 필요가 있습니다.    
우리는 $$\mathbb{S}={x| \forall i, g^{(i)}(x)=0 and \forall j,h^{(j)}(x) ≤ 0}$$가 되도록 m개의 함수 g(i)와 n개의 함수 h(j)의 관점에서 S를 기술하고자 합니다. 
- g(i)를 포함하는 방정식을 **equality constraints**이라고 하고 h(j)를 포함하는 부등식을 **inequality constraints**이라고 합니다.

우리는 각 제약 조건에 대한 새로운 변수 $\lambda_i$와 $\alpha_j$를 도입하며, 이를 KKT multipliers(승수)라고 합니다. 그러면 일반화된 Lagrangian은 다음과 같이 정의됩니다:

$$L(x,\lambda,\alpha)=f(x)+\sum_i \lambda_i g^{(i)}(x) + \sum_j \alpha_j h^{(j)}(x)$$

이제 우리는 일반화된 Lagrangian의 제약 없는 최적화를 사용하여 제약된 최소화 문제를 해결할 수 있습니다. 적어도 하나의 feasible point가 존재하고 f(x)가 값 ∞을 갖지 않는 한, 다음과 같습니다:

$$\min_x \max_\lambda \max_{\alpha,\alpha \le 0} L(x,\lambda,\alpha)$$

위의 식은 동일한 optimal objective function 값을 가집니다. 그리고 optimal points x를 다음과 같이 설정합니다:

$$\min_{x \in \mathbb{S}} f(x)$$

이는 constraints가 만족하면 다음을 따릅니다;

$$\max_\lambda \max_{\alpha,\alpha \le 0} L(x,\lambda,\alpha) = f(x)$$

constraints가을 위반할때면 다음을 따릅니다;

$$\max_\lambda \max_{\alpha,\alpha \le 0} L(x,\lambda,\alpha) = \inf$$

이러한 속성은 "infeasible point은 최적이 될 수 없으며, feasible points의 optimum은 변하지 않음"을 보장합니다.

constrained maximization를 수행하기 위해, -f(x)의 일반화된 Lagrange function를 구성할 수 있으며, 이는 다음과 같은 최적화 문제로 이어집니다:

$$\min_x \max_\lambda \max_{\alpha,\alpha \le 0} -f(x)+\sum_i \lambda_i g^{(i)}(x) + \sum_j \alpha_j h^{(j)}(x)$$

We may also convert this to a problem with maximization in the outer loop:

$$\max_x \min_\lambda \min_{\alpha,\alpha \le 0} f(x)+\sum_i \lambda_i g^{(i)}(x) - \sum_j \alpha_j h^{(j)}(x)$$

equality constraints에 대한 항의 부호는 중요하지 않습니다;   
최적화는 각 λi에 대한 임의의 부호를 자유롭게 선택할 수 있기 때문에, 원하는 대로 덧셈 또는 뺄셈으로 정의할 수 있습니다.

inequality constraints는 특히 흥미롭습니다. 우리는 $h^(i) (x^*) = 0$이면 제약 $h^(i) (x)$가 **active**하다고 말합니다. constraint이 active 하지 않은경우, constraint을 사용하여 찾은 문제의 solution은 해당 constraint가 제거된다면 적어도 local solution으로 남아있을 것입니다.    
inactive constraint은 다른 solution을 배제할 수 있습니다. 
- 예를 들어, globally optimal points의 전체 영역(넓고 평평한 비용의 영역)을 가진 convex problem는 constraints에 의해 이 영역의 subset이 제거되거나, non-convex 문제는 convergence에서의 inactive한 constraint에 의해 제외된 더 나은 local stationary points가 될 수 있습니다. 
그러나 convergence에서 찾은 점은 inactive constraints을 포함하는지 여부에 관계없이 stationary point로 남아있습니다. 
- inactive h^(i)는 음의 값을 가지므로, $\min_x \max_\lambda \max_(\alpha,\alpha \le 0) L(x, \lambda,\alpha)$ 의 해는 $\alpha_i = 0$ 을 가질 것입니다. 
- 따라서 우리는 해에서 $\alpha \odot h(x) = 0$ 을 관찰할 수 있습니다. 
다시 말해서, 모든 i에 대하여, 우리는 constraints $\alpha_i ≥ 0$ 와 $h^(i)(x) ≤ 0$ 중 적어도 하나가 해에서 active여야 함을 알고 있습니다.   
이 아이디어에 대한 직관을 얻기 위해서 우리는 해가 부등식에 의해 부과된 경계에 있고 해에 x의 영향을 미치기 위해 KKT 승수를 사용해야 하거나, 부등식이 해에 영향을 미치지 않고 이를 KKT 승수를 zeroing함으로써 나타낼 수 있습니다.

간단한 set of properties는 문제의 optimal points를 성명합니다. 이러한 성질을 Karush-Kuhn-Tucker (KKT) conditions이라고 합니다. 이들은 하나의 point가 optimal하기 위해 필요한 조건이지만, 하지만 항상 충분조건은 아닙니다. 조건은 다음과 같습니다;
- The gradient of the generalized Lagrangian is zero.
- All constraints on both x and the KKT multipliers are satisfied.
- The inequality constraints exhibit “complementary slackness”: $\alpha \odot h(x) = 0$ 

## EX) Linear Least Squares

다음 식을 최소화하는 x를 찾는다고 가정해봅시다.

$$f(x)={1 \over 2} {\left| Ax-b \right|}^2_2$$

이 문제를 효율적으로 풀수 있는 특별한 linear algebra algorithm이 있습니다. 그러나 우리는 지금껏 봐온 기술들의 예시로서 gradient-based optimization을 사용하여 어떻게 풀지도 알아봐야합니다. 

처음으로, 우리는 gradient를 얻어야 합니다.

$$\nabla_x f(x) = A^T(Ax-b)=A^TAx-A^Tb$$

우리는 작은 step을 통해 위의 gradient 내리막을 따를수 있습니다. 세부적으로 보기 위해 algorithm 4.1을 봅시다.

![figure6](/assets/img/posts/MIT_DL4/figure6.png)   

이 문제를 Newton's method를 사용해 풀수도 있습니다. 이경우에는 true function이 2차이기 떄문에, Netwon's method의 2차 근사티는 정확하며 알고리즘은 단 한번의 step으로 global minimum에 수렴합니다.

이제 같은 함수를 최소화하고 제약 $x^Tx ≤ 1$을 적용한다고 가정합시다. 이를 위해 Lagrangian을 도입합니다.

$$L(x,\lambda) = f(x)+\lambda(x^Tx-1)$$

우리는 이제 문제를 다음과 같이 풀수 있습니다.

$$\min_x \max_{\lambda,\lambda \le 0} L(x,\lambda)$$

unconstrained least squares problem의 smallest-norm solution은 oore-Penrose pseudoinverse: $x = A^+b$ 를 사용하여 찾을 수 있습니다. 
만약 이 point가 feasible하다면, 그것은 constrained problem의 해입니다. 그렇지 않으면, 우리는 constraint가 active한 해를 찾아야 합니다. Lagrangian을 x에 대하여 미분함으로써, 우리는 다음의 방정식을 얻을 수 있습니다

$$A^TAx-A^Tb+2\lambda x =0$$

위 식은 우리가 다음과 같은 form의 해를 갖게 될 것을 말합니다;

$$x= (A^TA+2\lambda I)^-1 A^T b$$

결과가 constraint를 따르도록 lambda의 크기를 선택해야 합니다. 우리는 lambda에서 gradient ascent를 수행함으로써 해를 구할 수 있습니다. 그러기 위해서는 다음을 확인해야합니다;

$$ {\partial \over \partial \lambda} L(x,\lambda) = x^T x -1$$

- x의 노름이 1을 초과할 때 이 도함수는 양수이므로, 이 도함수를 따라 λ에 대한 Lagrangian을 증가시키기 위해 λ을 증가시킵니다. 
- x^T x 패널티의 계수가 증가했기 때문에, x에 대한 일차방정식은 이제 더 작은 노름을 갖는 해를 산출할 것입니다. 
- 일차방정식을 풀고 λ을 조절하는 과정은 x가 정확한 노름을 갖고 λ에 대한 도함수가 0이 될 때까지 계속됩니다.
  
이로써 머신러닝 알고리즘 개발을 위한 수학적인 예습을 마치며, 본격적인 학습 시스템을 구축하고 분석할 준비를 마쳤습니다.