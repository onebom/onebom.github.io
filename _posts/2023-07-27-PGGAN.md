---
title: 끄적이며 알아보는 PGGAN(Progressive Growing of GAN) 
author: onebom
date: 2023-07-27 20:55:00 +0800
categories: [DL, ComputerVision]
tags: [GAN]
toc: true
---

이번 포스팅은 Tero Karras et al.이 ICLR 2018에서 발표한 "Progressive Growing of GANs for Improved Quality, Stability, and Variation"과 관련 (블로그 포스팅)[https://jjuon.tistory.com/41]을 요약한 글입니다.

## 1. Introduction
기존 GAN에서 고해상도 이미지를 생성하는것은 매우 어려웠다.
- 생성하는 이미지의 해상도를 높일 수록 real 이미지인 학습 이미지의 distribution과 generate된 fake 이미지의 distribution 차이가 커졌다.
- 자원문제로 인해, 일반적으로 고해상도 이미지를 훈련시키려면 적은 배채사이즈를 가지므로 불안정한 학습을 야기함

따라서 고해상도 이미지를 안정적으로 생성하기 위해,   
G,D를 저해상도부터 학습시켜 점점 layer를 추가하며 고해상도 학습까지 커지게하는 학습방법을 제안함  

> [Content summary]
> 3장: 생성된 이미지의 variation을 향상시키기 위한 방법
> 5장: 이미지 quality와 variation을 측정하기 위한 새로운 metric 제시
> 4.1절: 학습 속도 확보를 위한 방법
> 4.2절: mode collapse를 해결하기 위한 방법

## 2. Progressive Growing of GANs
본 논문의 핵심아이디어는 다음 그림과 같다;
!()[]
저해상도 이미지에서 시작해 위의 사진처럼 layer를 추가해가면서 고해상도에 도달하는 것이다.
- image distribution에서 큰 구조(coarse-grained) 특징을 우선 학습
- 점차 세밀한 특징(fine-grained)들을 이어서 학습

G와 D는 서로 반대되는 구조를 갖는다
!()[]
이때 주의할만 기법은 이전 학습된 기존 layer를 새 layer에 fade하는 방식이다.
- layer가 추가될떄, 낮은 해상도에서 높은 해상도로 갈수록 layer 가중치 α가 0에서 1로 선형적이게 증가하도록 한다. 
- 여기서 2x 및 0.5x는 nearest neighbor filtering과 average pooling을 사용하여 이뤄진다.
- toRGB는 특징 벡터를 RGB 색상으로 투사하는 레이어를 나타내며 fromRGB는 그 반대의 역할을 한다.
  
progressive training의 장점
1. 초기 학습에서 저해상도의 이미지를 학습하며 해상도를 조금씩 늘려가면서 학습하는 것이 1024x1024를 바로 학습하는 것 보다 훨씬 쉬운 문제다
2. 한번에 고해상도 이미지를 학습하는 것보다 최대 6배 학습속도 향상을 이뤄냈다 
 
## 3. Increasing Variation using Minibatch Stamdard Deviation
기존 GAN은 학습이미지에서의 variation만 포착하는 경향이 있었다.   
학습이미지에서만이 아닌, 실제 도메인의 분포를 따르기 위해 minibatch discrimination이 제안되었다.   
- 각 이미지에서 뿐만 아니라 minibatch에서도 feature sytatistics를 계산하여 실제 도메인에서의 이미지의 statistic을 배울 수 있도록 했다.

[설계과정]
1. 우선, minibatch에 대해 각각의 spatial location의 feature의 std(표준편차)를 계산한다
2. 그 다음에 이 std를 모든 feature와 spatial location에 대해 평균을 내어 하나의 single value로 만든다. 
3. 이를 minibatch의 모든 spatial location에 대해 복제하고 concatenate하여 하나의 constant feature map을 만든다.
4. 이 map은 discriminator에 어디에 넣어도 좋지만, 가장 마지막에 넣는 것이 가장 좋다.

## 4. Normalization in Generator and Discriminator
GAN은 G,D의 학습속도 차이로 인해 학습의 어려움을 겪는다.
본 논문에서는 1)equalized lr, 2)pixelwise feature vector noramlization으로 이러한 signal을 직접적으로 규제한다;
### 4-1. Equalized Learning Rate
단순히 standard normal distribution으로 weight을 초기화하고, 실행중에 weigth을 $\hat{w_i}=w_i/c$로 scaling 한다.
- c: he initialization의 per-layer normalization constant다.
- 처음부터 scaling하지않고 학습중에 동적으로 수행하는 이유는 RMSProp 및 Adam과 같은 일반적으로 사용되는 SGD 방식의 scale-invariance와 관련있다.
  - 즉, 동적으로 수행함으로써 std만큼 gradient update를 noramlize하므로 => parameter scale과 무관하게 update를 할 수 있다.

### 4-2. Pixelwise Feature Vector Normalization in Generator
G,D의 gradient 통제를 벗어나는 경우를 방지하기 위해(spiral out of control);   
feature vector의 단위 길이 만큼 각 pixel을 noramlize한다.
- AlexNet에서 소개된 local response normalization을 변형하여 구현했다.

## 5. Multi-scale Statistical Similarity for Assessing GAN Results

GAN이 큰 규모의 mode collapse는 잘 포착하지만 variation의 손실이나 작은 변화는 잘 포착하지 못한다.
따라서 (Laplcian pyramid)[https://dsbook.tistory.com/219#:~:text=%EB%9D%BC%ED%94%8C%EB%9D%BC%EC%8B%9C%EC%95%88%20%ED%94%BC%EB%9D%BC%EB%AF%B8%EB%93%9C%20(Laplacian%20Pyramids),%EC%95%95%EC%B6%95%EC%97%90%20%EC%A3%BC%EB%A1%9C%20%EC%82%AC%EC%9A%A9%EB%90%9C%EB%8B%A4.]를 활용해 작은 변화를 알아낸다.
- 2^14개의 이미지를 sampling하고 pyramid의 각 level에서 2^7개의 descriptor를 추출한다.
-  각 discriptor는 7x7에 RGB 3채널로 이뤄져있고, dimesion이 147이다.
-  pyramid의 l번째 feature에서 실제 이미지와 생성된 이미지의 패치에 대해, 채널별로 noramlize한 후 sliced Wassetein distance(SWD)를 구한다.
-  SWD가 작으면 두 패치의 distribution이 비슷하다 뜻이며, 해당 resolution에서 appearnace와 variaton이 모두 비슷하다고 볼 수 있다.

## 6. Experiments
progressive growing 성능이 좋다