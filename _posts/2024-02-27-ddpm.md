---
title: 끄적이며 정리하는 Diffusion Model
author: onebom
date: 2024-02-27 00:00:00 +0800
categories: []
tags: []
toc: true
published: true
math: true
---

## 1. What is a Diffusion Model
diffusion model의 시작은 논문 abstarct에 쓰여있듯이 non-equilibrium thermodynamics(비평형열역학)이다. 
[권민기님](https://www.youtube.com/watch?v=uFoGaIVHfoE&list=PLMt_iGrhm2UvSXBM7SmngPB9gtMQpy-CN&index=1)의 비평형 열역학에 대한 physical intuition 설명에따르면,   
> [non-equilibrium thermodynamics]   
분자에 diffusion을 가하게 되면, 본래 분자모임의 struct가 망가진다. 지속해서 diffusion이 가해져 시간이 지나면, 결국 분자는 공간에 uniform하게 분포되게 될것이다. 따라서 우리가 잘 알고있는 uniform에서 시작해 처음 상태로 돌아갈 수 있다면, diffusion이 가해지기 전 최초의 분자 상태를 알아낼 수 있지 않을까? 이를 딥러닝으로 해결하고자 한것이다.    
![figure0](/assets/img/posts/ddpm/figure0.gif) 
- 열역학에서 분자들의 다음 위치는 가우시안 분포 안에서 결정된다.
- 또한, 정말 작은 sequence에서 diffusion 움직임은 foward와 reverse 동일하다고 가정한다. ????

diffusion model은 Markov chain에 기반하여 데이터에 random noise를 천천히 추가하는 diffusion 단계를 정의하고, 완전 노이즈로부터 원하는 데이터 샘플을 구성하는 diffusion reverse 단계를 학습한다.   

다른 생성모델과 달리, latent variable이 원본데이터와 동일한 높은 dimensionality를 가진다.
![figure1](/assets/img/posts/ddpm/figure1.png)

!! diffusion model vs ddpm 간단하게

DDPM paper Abstarct의 아래 두줄에 집중해서 본 post를 읽어볼 것이다. 
> diffusion probabilistic model과 denoising score matching with Langevin dtnamics를 연결한 weighted variational bound를 통해 좋은 학습결과를 냈다.
> 또한, autoregressive deocding을 일반화한 progressive lossy decompression scheme을 제안한다.

워낙 생소한 단어로 abstract에서 모델을 설명하고 있기에, 이를 간단하게 설명하고자 한다.   
따라서 본 글을 읽을땐, 아래의 질문3개를 기억하며 읽어주길 바란다.

<details>
<summary>**[QnA]**</summary>
<div markdown="1">
1. **비평형 열역학과 DDPM의 공통점은?** : 
    작은 time sequence에서 diffusion의 foward와 reverse가 모두 가우시안이라는 점을 사용하여, 비평혁 열역학이 uniform한 분자분포에서 diffusion 이전의 본래 분자 상태를 알아내듯이 diffusion model도 마찬가지로 데이터에 gaussian noise를 가하는 foward process의 역을 알아내 gaussian noise가 주어졌을 때 원본 데이터를 구한다.
2. **weithted variational bound가 무엇을 말하는 지** :
3. **progressive lossy decompression이 어떻게 일어나는지** :
</div>
</details>


## 2. How does it work?

> A diffusion probabilistic model(which we will call a “diffusion model” for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time

### (1) Foward diffusion process
먼저, 데이터에 노이즈를 더해가며 완전 gaussian distribution의 lantent variable을 만들어내는 foward process에 대해 알아보자.   

![figure3](/assets/img/posts/ddpm/figure3.PNG)
실제 데이터 분포에서 샘플링한 data point $x_0 \sim q(x)$가 주어졌을 때, 해당 샘플에 T스텝만큼 소량의 Gaussian noise를 추가하며 $x_1,...,x_T$ noisy smaples sequence를 얻는다.
- 이때 각 step의 size는 variance schdule에 의해 0에서 1사이로 조정된다: ${\beta_{t} \in (0,1)}^{T}_{t=1}$

data sample x_0은 점진적으로 t가 커짐에 따라 distinguishable features들을 잃는다. 결과적으로 T가 무한에 수렴하게 되면 x_T는 완전 Gaussian distribution을 따르게 된다.   
**x_{t-1}로부터 노이즈를 더해 x_t를 구하는 과정**을 식으로 표현하면 다음과 같다;   

$$q(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_{t}\mathbf{I}) \rightarrow q(x_{1:T}|x_0)=\prod_{t=1}^T q(x_t|x_{t-1}) $$

- x_t를 정의할때 x_{t-1}을 사용한 조건부확률분포로 표현할 수 있고, x_t는 가우시안 정규분포를 따른다.(왜?????)
- 정규분포항을 살펴보자:    
    x_t의 평균이 x_{t-1}에 비례하되, 1-beta_t가 곱함으로써 이전상태의 영향을 감소시켰다. 대신, 분산에 beta_t를 둠으로써 beta_t 값에 따라 노이즈를 더한다.
    - 뒤에서 설명하겠지만, beta_t 매개변수는 시간에 따라 값이 변동하도록 설정해둔다.
    <details>
    <summary>varaince 관점에서 x_t의 정규분포</summary>
    <div markdown="1">
    왜 x_t가 저런 정규분포 식을 갖는지 궁금할 것이다. 안 궁금하면 넘어가도된다.    
    결론부터 말하면, Variance를 1로 유지하기 위함이다.    

    분산의 누적과정을 추적해보자. x_t의 분산은 다음과 같이 표현될 수 있다;

    $$Var(x_t) = Var(\sqrt{1-\beta_{t}} x_{t-1} + \epsilon_{t})$$

    - 여기서 epsilon은 평균이 0이고 분산이 beta_t인 정규분포에서의 노이즈를 나타낸다. 

    마저 전개하면,

    $$Var(x_t)=(\sqrt{1-\beta_{t}})^2 Var(x_{t-1})+ Var(\epsilon_{t})$$
    $$Var(x_t)=(1-\beta_{t})Var(x_{t-1})+\beta_t$$

    t-1에서의 분산이 1이라고 가정한다면, 식을 다음과 같이 간단하게 만들수 있다;

    $$Var(x_t)=(1-\beta_{t}) + \beta_{t} = 1$$

    위와 같은 조건하에, 다음 step으로 넘어가도 분산이 1로 일정하게 유지할 수 있다.

    <hr/>
    </div>
    </details>

- 우항에 대해 살펴보면, Markov chain에 따라 $q(x_{1:T}|x_0)$는 T시점까지 $q(x_t \| x_{t-1})$의 joint distribution으로 나타낼 수 있다.
    ![figure8](/assets/img/posts/ddpm/figure8.jpg)



#### Diffusion Kernel
지금까지, diffusion forward process가 원본데이터에 noise를 조금씩 여러 step에 더해나가며 완전한 gaussian noise을 만든다는 개념에 대해 이해했다. 그렇다면, noise를 조금씩 단계별로 주는것 대신 **특정 time step에 대해 noise를 한번에 줄 수 있지 않을까?**   
foward diffusion process는 각 step에 대해 가우시안 커널들이 연속적으로 이뤄지기 때문에, 특정 step t에 대한 가우시안 커널을 한번에 정의할 수 있다. 그리고 이것이 기존의 diffusion model과 달리 ddpm이 가지는 특징이다. (맞는말인지 확인하기)
![figure4](/assets/img/posts/ddpm/figure4.PNG)

해당 내용에 대해 paper에 아래와 같이 적혀있다.
> 이 process의 장점은 reparameterization trick을 사용하여, 임의의 시간 step t에 대해 *닫혀있도록* x_t를 sampling할 수 있어, explicit한 데이터 표현이 가능하다.   

이 문장에 대해 분석해보면, 임의의 시간 step t에 대해 x_t가 닫혀있도록 sampling한다는 표현은, 어떠한 시간 단계 t에서도 x_t를 직접얻어낼 수 있다는 말이다.   
즉, 중간 단계의 샘플링이 없어도 x_t 값을 계산할 수 있다는 것인데, 이것은 reparameterization trick으로 x_t의 노이즈항을 재매개변수화했기 때문이다.   

다시 쉽게 설명하자면, **기존의 x_t 식에 포함되어 있던 미분이 불가능한 노이즈 항을 N(0,I)에서 샘플링하도록 정의함으로써 t까지의 (1-beta) 값들의 누적곱으로 재매개변수화하여 미분 가능한 연산으로 변환했기 때문에 어떠한 t에 대해서도 x_t를 직접 구할 수 있다**라는 뜻이다.   

<details>
<summary>식전개로 reparameterization 과정을 쉽게 알아보자</summary>
<div markdown="1">
기존의 x_t 식은 $N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_{t}\mathbf{I})$ 정규분포를 따르기 때문에 다음과 같이 쓸수 있다;

$$x_t=\sqrt{1-\beta_t}x_{t-1}+\beta_{t}\epsilon$$
- epsilon은 N(0,I)를 따르는 노이즈항이다.

여기서, 우리는 시점 t에 대해서 t 이전 step의 beta를 모두 product한 항을 정의하여 사용할 것이다;

$\alpha_{t}=1-\beta_{t}$ and $\bar{\alpha_{t}}=\prod_{i=1}^t \alpha_{i}$

위의 alpha 값을 사용해 기존의 x_t 식을 치환한다면,

$x_t = \sqrt{\alpha_{t}}x_{t-1}+\sqrt{1-\alpha_{t}}\epsilon_{t-1}$

이때 x_{t-1}은 x_{t-2},..., x_1은 x_0으로 표현될 수 있기 때문에, 차례차례 전개할 수 있다.

![figure5](/assets/img/posts/ddpm/figure5.jpg)

$= \sqrt{\alpha_{t}\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha{t}\alpha{t-1}}\bar{\epsilon}_{t-2}$

$= ... $

$= \sqrt{\bar{\alpha_{t}}}x_0 + \sqrt{1-\bar{\alpha_{t}}}\epsilon $

결과적으로 x_0이 주어졌을 때, $q(x_t \| x_0)$은 다음과 같이 가우시안으로 나타낼 수 있다

$$q(x_t \| x_0)= N(x_t; \sqrt{\bar{\alpha_{t}}}x_0, (1-\bar{\alpha_{t}})\mathbf{I})$$

<hr>
</div>
</details>     

<br>


기존의 beta를 alpha항으로 치환해 $q(x_t\|x_{t-1})$의 누적곱으로 $$q(x_t \| x_0)$$을 다음과 같이 표현했다;

$$q(x_t \| x_0)= N(x_t; \sqrt{\bar{\alpha_{t}}}x_0, (1-\bar{\alpha_{t}})\mathbf{I})$$

- 이때 beta값은 T에 가까워질 수록 값이 커지도록 설정한다. 따라서, x_0에서 x_T에 가까워질 수록 이전 스텝에서 노이즈를 추가하는 비율이 커지게된다.

결과적으로, x_0만 있다면 어떤 임의의 t든간에 noise낀 x_t를 구해낼 수 있게 되었고 이를 통해 기존의 좋지않았던 diffusion model의 학습 성능을 높이게 되면서 DDPM 이후 diffusion model을 본격적으로 사용하기 시작했다.   
학습 성능을 높일 수 있던 이유에 대해서는 !!!!!!에서 설명하겠다.   


#### What about q(x_t)
지금까지, 우리는 diffusion kerenl $$q(x_t|x_0)$$에 대해 알아보았다. 이제, diffusion kernel을 통해 q(x_t)를 이해할 수 있다.
![figure6](/assets/img/posts/ddpm/figure6.PNG)
- 위 figure에도 나와있듯이, q(x_t)는 q(x_0)과q(x_t)의 joint distribution으로 나타낼 수 있으며, q(x_0)에 gaussian convolution을 곱한거와 같다.


다른 기존의 diffusion model은 모델을 학습시킬때 x_0으로부터 전체 x_1~x_T의 diffusion 정보를 담은 posteriror $$q(x_{1:T}|x_0)$$에 reverse diffusion process를 근사시키는 것이 목표였다.   
![figure2](/assets/img/posts/ddpm/figure2.png)
- 즉, 노이즈로부터 데이터를 생성하는 $$q(x_{t-1}\|x_t)$$을 알아내기 위해, $$q(x_t\|x_{t-1})$$을 알고있으므로 모델 $$p_\theta(x_{t-1}\|x_t)$$를 $$q(x_t\|x_{t-1})$$에 근사시키는 방법이다.
- p를 q에 근사함으로써, $$q(x_{t-1}\|x_t)$$을 알아낼 수 있는 이유는 $$\beta_{t}$$가 아주 작은 size라면, 둘은 동일시 되기 때문이다. => 위에서 언급한 비평형 열역학의 특징 : "작은 time sequence에서 diffusion의 foward와 reverse가 모두 가우시안이라는 점" 

!! DDPM에서는 어떻게 하는가

### (2) Reverse diffusion process
gaussian noise input으로 실제 샘플 $$x_t \sim~ N(0,I)$$를 다시 생성하는 과정을 reverse diffusion process라 한다. diffusion 과정의 각 단계가 작은 time sequence에서 이뤄진다고 가정했을때, reverse process를 gaussian으로 모델링할 수 있고 그렇기에 해당 과정을 딥러닝 모델이 학습하도록 할 수 있다.   
![figure7](/assets/img/posts/ddpm/figure7.PNG)

diffusion kernel $q(x_t\|x_{t-1})$의 역을 근사하기 위한 모델 $p_\theta(x_{t-1}\|x_t)$는 가우시안 정규분포를 따른다고 가정해뒀기 때문에, 평균 $\mu$와 분산 $\sigma$만 안다면 데이터 분포를 추정할 수 있다=> 따라서 딥러닝 모델은 평균과 ~~분산~~을 학습해야한다.

**DDPM의 가우시안 노이즈로부터 데이터를 생성하는 variational autoencoder를 학습시키는 과정은 아래의 negeative log likelihood에서의 variational upper bound를 최적화하는 과정이다;**   
(이제부터, DDPM의 loss term을 도출해내고 이를 증명할 것이다. 단순히 DDPM의 모델링부분만 알아가고 싶다면 (4)로 넘어가라)

$$E_{q(x_0)} \left[ - \log p_{\theta}(x_0) \right] \leq E_{q(x_0)q(x_{1:T}|x_0)} \left[ - \log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)} \right] =: L$$

$$L=E_q[\underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_T \mid \mathbf{x}_0\right) \| p\left(\mathbf{x}_T\right)\right)}_{L_T}+\sum_{t>1} \underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)\right)}_{L_{t-1}} \underbrace{-\log p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_1\right)}_{L_0}]$$

먼말인지 하나도 몰라도 되고, 수식은 흐린눈으로 봐도 된다.   
지금부터 차근차근 설명해주겠다.   

<details>
<summary>우선, 잘알고 있는 bayes's rule을 통해 negative log likelihood에 대해서 집고넘어가자</summary>
<div markdown="1">
알고있는 개념이라면 넘어가도 된다.
[likelihood And Bayes's Rule]
![figure10](/assets/img/posts/ddpm/figure10.png)

- H: hypothesis로 추정하고자 하는 값이다. => prior은 추정하려는 값의 확률분포를 말한다
- D: observation으로 관측된 데이터를 말한다. 예를 들어 training data가 될 수 있다. => evidence는 데이터 자체의 분포를 뜻한다.
- Posterior: observation이 주어졌을 때의 H의 분포이다.
- likelihood: H가 주어졌을 때(가정을 한 상태)에서의 데이터의 확률분포를 말한다.

좀더, 예시를 들어 직관적으로 설명한다면   
![figure11](/assets/img/posts/ddpm/figure11.png)   
위와 같이 연속형 확률변수를 나타낼 때는 확률밀도함수 PDF로 표현한다.    
예시에서처럼, 고양이 몸무게 분포인 PDF는 고정되어 있고 4kg 이상 혹은 5kg 이상 처럼 입력데이터가 변할때의 해당 입력의 확률을 "Posterior"라고 한다.   
![figure12](/assets/img/posts/ddpm/figure12.png) 
위의 예시에선, 고양이 몸무게가 5kg로 고정되어있고 분포가 변하는 상황이며, 이때의 입력의 확률을 "likelihood"라 한다. 따라서, 입력이 주어졌을때 분포가 데이터를 얼마나 잘 설명하는가에 대한 지표가 된다. => 데이터들이 주어졌을때, 데이터를 가장 잘 설명할 수 있는 분포를 찾는 것인 딥러닝의 목적과 동일하다.


딥러닝의 관점에서 이해하자면,   
![figure13](/assets/img/posts/ddpm/figure13.PNG) 
mnist데이터 중 1이 그려진 이미지는 위와 같은 posterior값을 가진다. 이후 딥러닝 모델을 구현하여 class 확률값을 뽑았을때 아래의 분포들 중 정답과 가까운 분포는 model C의 distribution이다.    
![figure14](/assets/img/posts/ddpm/figure14.PNG) 
즉, likelihood가 가장 높은 분포다.

따라서 딥러닝 모델은 분포를 잘 학습해서 입력값들의 likelihood를 최대화시키는것이 목적이다. 이것이 바로 Maximum log likelihood estimation(MLE)이다.   

[Negative Log-liklihood]   
딥러닝을 잘 학습시키기 위해서는 모든 입력값에 대한 likelihood의 결합을 최대화하면 되고 목적함수는 다음과 같다;
$$ \arg \max_\theta f(x\|\theta) = \arg \max_\theta \{f(x\|\theta_{1}) \cdot f(x\|\theta_{1}) \cdots f(x\|\theta_{n}) \} $$ 

여기서, likelihood function은 다음과 같이 표현한다;

$ L(\theta) = p_\theta(x) = f(x\|\theta_{1}) \dot f(x\|\theta_{1}) \cdots f(x\|\theta_{n})$
$ = \prod_{i=1}^n f(x\|\theta_{n}) $

이때 likelihood function은 곱셈의 형태이다. 곱셈으로 이뤄진 복잡한 형태를 **덧셈으로 바꿔주기 위해 log를 취하게된다**

$log L(\theta) = log \prod_{i=1}^n f(x\|\theta_{n}) = \sum_{i=1}^n log f(x\|\theta_{i}) $

이를, Log-likelihood라 한다.    
곱셈의 형태에서 덧셈으로 바꿨을 때 이점들은 [이분의 블로그](https://blog.naver.com/PostView.naver?blogId=ycpiglet&logNo=223110695364)를 확인해주길 바란다.   

마지막으로, 딥러닝의 경사하강법을 사용하기위해 log-likelihood에 음수를 곱해주어 maximum 문제를 minimum문제로 바꾼다.

<hr>
</div>
</details>

<br>

확률분포를 추정하는 모델은 입력데이터(observation)의 negative log-likelihood(NLL)인 $-log p_\theta(x)$를 최소화함으로써 학습을 최적화할 수 있다. 따라서, reverse diffusion process의 모델은 각 step에서의 generation에 필요한 모든 observation인 x_1~x_T에 대해 NLL을 최소화해야한다.   

#### VAE Autoencoder optimization with ELBO
기존 VAE모델의 방법을 빌려오자면, low-dimension의 latent space($p(x_T)$)를 사용해 high-dimension의 데이터 분포($p_\theta(x)$, likelihood)를 표현하는 방법은 2가지다.
1. $p_\theta(x)$에 대해서 $p(x_T)$ (latnet variable,z)의 **chain rule probability**로 표현하는 것
    - Bayes Rule의 조건부 확률식을 이용
    - $p(x)={p(x,z) \over p(z\|x)}$ (theta 생략)

2. $p_\theta(x)$에 대해서 $p(x_T)$ (latnet variable,z)를 **marginalize**하여 표현하는 것
    - latent variable의 확률밀도함수의 성질과 주변확률분포 이용
    - $p(x)=\int p(x,z),dz$ (theta 생략)

이제 위에서 제시한 두식을 가지고 -log를 씌워 가장 작은 값을 구하면 될거 같지만, 두 식에서 직접적으로 NLL을 최적화하는 것은 불가능하다. 첫번째 방법에서는 ground truth인 $p(z\|x)$를 알아야만 하고, 두번째 방법에서는 모든 latent variable z에 대해서 통합시킨것으로 연산해야하기 때문이다.

**따라서 ELBO를 도입해, $q(x)$라는 다루기 쉬운 분포를 가정하고 KLD를 통해 $p(z\|x)$와 $q(x)$의 분포 차이를 최소화함으로써 NLL을 최적화할것이다.**

<details>
<summary>[ELBO: Evidence of Lower BOund]</summary>
<div markdown="1">
ELBO는 'variational lower bound'라고도 불리며, 관찰한 분포 $p(z\|x)$가 다루기 힘든 분포를 이루고 있을때, 조금 더 다루기 쉬운 분포( ex. 가우시안 분포)인 q(x)로 대신 표현하는 과정에서 두분포의 차이(KL Divergence)를 최소화하기 위해 사용된다.   
KLD로부터 식을 살펴보면;   
$D_{\mathrm{KL}} (q(z)\|p(z\|x)) = \sum q(z) \log({ q(z) \over p(z\|x)}) $ ... CrossEntropy항
$= \sum q(z) \log q(z) - \sum q(z) \log p(z\|x)$
$= E_q[\log q(z)] - E_q[\log p(z\|x)] = E_q[\log q(z)] - E_q[\log {p(x,z) \over p(x)}] $ ... Bayes' Rule
$= E_q[\log q(z)] - E_q[\log p(x,z)] - E_q[\log p(x)]$
$= E_q[\log q(z)] - E_q[\log p(x,z)] - \log p(x)$ ... Expectation 정의

> [Expectation 정의]   
> $E_q[\log p(x)] = \int q(z)\log p(z), dz = \log p(x) \int q(z),dz = \log p(x)$   
 
마지막 식을 log p(x)를 기준으로 변경하면;
$ \log p(x) = D_KL (q(z)\|p(z\|x)) - E_q[\log q(z)] + E_q[\log p(x,z)]$   

KLD는 항상 0보다 크거나 같으므로,
$ \log p(x) \le D_KL (q(z)\|p(z\|x)) + ELBO$   

여기서 $E_q[\log p(x,z)]- E_q[\log q(z)]$ 항을 ELBO라고 한다.   
결국, p(x)는 constant로 변하지 않는 값이기에 ELBO가 증가하면 KLD는 감소하므로, ELBO를 최대화함으로 분포간 차이를 최소화할 수 있다.
<hr>
</div>
</details>

<br>

위에서 제시한 두방법의 식을 이러한 성질의 ELBO 식으로 유도하여 NLL minimum 최적화 문제를 p(x,z)와 q(z)에 대한 ELBO 최대화 문제로 치환할 것이다.

<details>
<summary> likelihood 식을 ELBO식으로 유도하기</summary>
<div markdown="1">

#### $p_\theta(x)$에 대해서 $p(x_T)$ (latnet variable,z)의 **chain rule probability**로 표현해보자

$\log p(x) =\log p(x) \int q_\phi(z \mid x) d z$
- log p(x)에 $\int q(z) dz = 1$이므로 적분항을 곱해준다.

$=\int q_\phi(z \mid x)(\log p(x)) dz = E_{q_\phi(x \| z)}[\log p(x)]$ ... expectation 정의   

$=E_{q_\phi(x \mid z)}\left[\log \frac{p(x, z)}{p(z \mid x)}\right]$ ... Bayes' Rule   

$=E_{q_\phi(x \mid z)}\left[\log \frac{p(x, z) q_\phi(z \mid x)}{p(z \mid x) q_\phi(z \mid x)}\right]
=E_{q_\phi(x \mid z)}\left[\log \frac{p(x, z)}{q_\phi(z \mid x)}\right]+E_{q_*(x \mid z)}\left[\log \frac{q_\phi(z \mid x)}{p(z \mid x)}\right] $   
$=E_{q_\phi(x \mid z)}\left[\log \frac{p(x, z)}{q_\phi(z \mid x)}\right]+D_{\mathrm{KL}}\left(q_\phi(z \mid x) \| p(z \mid x)\right) $ ... KL Divergence

$\geq E_{q_\phi(x \mid z)}\left[\log \frac{p(x, z)}{q_\phi(z \mid x)}\right]$

#### $p_\theta(x)$에 대해서 $p(x_T)$ (latnet variable,z)를 **marginalize**하여 표현해보자

> [joint probability & marginal probability (결합확률과 주변확률)]
> latent variable은 데이터 생성에 영향을 미친다. 따라서 latent variable을 z라 했을때, x와 z의 결합분포를 정의할 수 있다.
이때 모든 z의 분포를 포함하기 힘들기때문에, z에 관계없이 x의 분포를 표현하고 싶으므로 주변확률의 정의에 따라 아래와 같이 p(x)를 표현할 수 있다;

$\log p(x) =\log \int p(x, z) d z $   
$=\log \int \frac{p(x, z) q_\phi(z \mid x)}{q_\phi(z \mid x)} d z $   
$=\log E_{q_\phi(x \mid z)}\left[\frac{p(x, z)}{q_\phi(z \mid x)}\right] $ ... Expectation 정의   

$\geq E_{q *(x \mid z)}\left[\log \frac{p(x, z)}{q_\phi(z \mid x)}\right]$ ... Jensen's Inequality

> [Jensen's Inequality]   
> ???

결과적으로, 두 방법모두 ELBO로 유도할 수 있음을 증명할 수 있다.

<hr>
</div>
</details>

<br>

ELBO를 통해 우리가 가진 p(x,z)와 q(zㅣx)만으로 p_theta(x)를 최적화할수 있게 되었다.
ELBO 항을 다시 들여다보면,   
$E_{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log \frac{p(\boldsymbol{x}, \boldsymbol{z})}{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\right]=E_{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log \frac{p_\theta(\boldsymbol{x} \mid \boldsymbol{z}) p(\boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z} \mid \boldsymbol{x})}\right] $ ... Chain Rule
$=E_{q_\phi(z \mid x)}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid z)\right]+E_{q_\phi(z \mid x)}\left[\log \frac{p(z)}{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\right] $ 
$=E_{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log p_{\boldsymbol{\theta}}(\boldsymbol{x} \mid \boldsymbol{z})\right]-D_{\mathrm{KL}}\left(q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \| p(\boldsymbol{z})\right)$

해당 전개에 따라 ELBO를 $p_\theta(x\| z)$에 대한 reconstruction term과 $q(z \| x)$와 $q(z)$에 관한 KL diversence 항인 prior matching term으로 나눌 수 있다
- reconstruction term은 decoder가 latent z로부터 observation 데이터를 얼마나 잘 복원하도록 학습되었는지를 측정한다: decoder 성능측정
- prior matching term은 학습된 variational distribution이 prior과 어마나 유사한지를 측정한다: encoder 성능 측정

#### ELBO to Variational Diffusion Model
지금까지 VAE에서 likelihood $q_\theta(x)$를 최적화하기 위해 ELBO식을 어떻게 유도하는지 알아보았다.   
이제 VAE에서의 ELBO를 DDPM의 상황에 맞춰 변형해볼 것이다;   
1. 하나의 encoder,decoder를 가지는 VAE와 달리, 여러개의 encoder와 decoder가 Hierarchical하게 연결되어있는 형식이다.   
2. lower-dimension의 latent variable을 가지던 VAE와 달리,latent dimension은 data dimension과 동일하다.
3. encoder를 함께 학습시키던 VAE와 달리, encode를 학습시키지 않고 선형 가우시안 모델로 고정한다.

첫번째로, VAE에서 도출했던 ELBO 식에서의 단일 z를 우리는 이제 각 step에 해당하는 latent의 joint distribution으로 이뤄진 $x_{1:T}$로 다룰것이다.   

$E_{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[\log \frac{p(\boldsymbol{x}, \boldsymbol{z})}{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\right] \rightarrow E_{q(x_{1:T} \mid x_0)}\left[\log \frac{p(x_{0:T})}{q(x_{1:T} \mid x_0)}\right]$   

두번째로, Markov chain을 따르는 아래의 foward process 식과 reverse process 식을 이용해 ELBO 식을 다르게 전개할 수 있다;  
- $q(x_{1:T}\|x_0) = \prod_{t=1}^T q_(x_t\|x_{t-1})$ 
- $p(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}\|x_t)$
<details>
<summary>reverse process 식의 좌항이 우항과 같음이 성립되는 이유: Markov chain</summary>
<div markdown="1">
위에서 Markov chain을 이용하여  foward process에 대한 식을 전개해보았었다.   
마찬가지로, reverse process에 대한 것도 동일한 개념을 이용해 전개할 수 있다.   
![figure9](/assets/img/posts/ddpm/figure9.jpg)
$$p_\theta(x_{0:T})$$를 x_T에서 시작해서 #$$p_\theta(x_{t-1}\|x_t)$$의 joint distribution으로 표현할 수 있다
<hr>
</div>
</details>
<br>

> foward process에서 $q(x_t\|x_{t-1})=N(x_t;\sqrt{\alpha_{t}}x_{t-1},(1-\alpha_t)\mathbf{I})$로 정의했다는 것을 상기시키자.     
> 또한 p(x_T)가 standard gaussian distribution $N(x_T; 0,\mathbf{I})$인것도 말이다.   
> 따라서 reverse process의 $p(x_{t-1}\|x_t)$는 gaussian distribution을 따르는 $N(x_{t-1}:\mu_\theta (x_t,t), \sigma_t^2 \mathbf{I})$로 정의한다.

$$\log p(x) \geq E_{q(x_{1:T} \mid x_0)}\left[\log \frac{p(x_{0:T})}{q(x_{1:T} \mid x_0)}\right] =E_{q\left(x_{1: T} \mid x_0\right)}\left[\log \frac{p\left(x_T\right) \prod_{t-1}^T p \boldsymbol{\theta}\left(x_{t-1} \mid x_t\right)}{\prod_{t=1}^T q\left(x_t \mid x_{t-1}\right)}\right]$$

<details>
<summary>위식의 생략과정</summary>
<div markdown="1">
$\log p(x) = \log \int p(x_{0:T}) d x_{1:T}$ ... marginalize   
$=\log \int {p(x_{0:T})q(x_{1:T}\|x_0) \over q(x_{1:T}\|x_0)} d x_{1:T}$   
$= \log E_{q(x_{1:T} \mid x_0)}\left[\log \frac{p(x_{0:T})}{q(x_{1:T} \mid x_0)}\right]$   
$\geq E_{q(x_{1:T} \mid x_0)}\left[\log \frac{p(x_{0:T})}{q(x_{1:T} \mid x_0)}\right]$ ... Jensen's Inequality   
<hr>
</div>
</details>
<br>

위에서 흐린눈으로 보고왔던 두식 중 첫번째 식에 도달했다!!
> $$E \left[ - \log p_{\theta}(x_0) \right] \leq E_q \left[ - \log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)} \right] = E_q \left[ - \log p(x_T) - \sum_{t \geq 1} \log {p_\theta (x_{t-1}\|x_t) \over q(x_t\|x_{t-1})} \right] =: L$$   

- prod 항을 log에대한 덧셈 항으로 전환하면 위와 동일해진다.   

다시 한번 정리하고 넘어가자면, Diffusion model에서 negative log likelihood를 최소화하는 문제를 ELBO를 최대화하는 쉬운 문제로 치환할 수 있었고 이를 증명했다!   


이제, 우리가 알고있는 변수들의 loss term들로 식을 구성하기 위해 ELBO를 쭉 전개해볼 것이다;
- DDPM에서는 Markov chain property를 통해 $q(x_t\|x_{t-1})=q(x_t\|x_{t-1},x_0)$을 사용할 수 있다.
![figure15](/assets/img/posts/ddpm/figure15.jpg)   

이제 드디어 위에 흐린눈으로 보고왔던 두개의 식 모두 쭉 알아봤다!! 박수 짝짝!

$$L=E_q[\underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_T \mid \mathbf{x}_0\right) \| p\left(\mathbf{x}_T\right)\right)}_{L_T}+\sum_{t>1} \underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)\right)}_{L_{t-1}} \underbrace{-\log p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_1\right)}_{L_0}]$$     
 
- reconstruction term: 
- prior matching term: 
- denoising matching term: 


### (3) tractable posterior distribution $q(x_{t-1}\|x_t,x_0)$ and Parameterizing DDPM



### (4) Training&Sampling Design and Network Architectures


## 3. Why does it work wll?
### Mathemetical Relationship with SDE and Score Matching

## 4. Advanced Fast Sampling: DDImplicitM


