---
title: GAN 톺아보기(GAN, DCGAN, WGAN)
author: onebom
date: 2023-07-14 20:55:00 +0800
categories: [DL, ComputerVision]
tags: [GAN]
toc: true
---

## What is GAN(Generative Adversarial Network)?

> How can a computer create photos by itself?   
> lan Goodfellow: What if you pitted two neural networks against each other?

- [Befor GAN](https://hyunsooworld.tistory.com/entry/최대한-쉽게-설명한-GAN) 

### [Summary]
생성모델G와 판별모델D의 상호 경쟁과정을 통해 이미지 feature 분포를 학습하여 학습 데이터의 평균양상을 띄는 이미지를 생성할수 있도록하는 학습 프레임워크이다.
- Generative model(생성모델G) : *captures the data distribution*, 학습 데이터의 분포와 근사한 데이터 생성
- Discriminative model(판별모델D) : G에서 생성된 가짜 데이터와 학습하는 진짜 데이터를 분류

결과적으로 G모델이 학습데이터의 분포를 묘사할 수 있게되면, D모델은 실제 학습데이터인지 G가 생성한 가짜 데이터인지 맞출 확률은 1/2에 수렴한다.   
즉, 잘 학습되어진 D가 G로부터 생성된 데이터를 가짜라고 판별하지 못하게 하는 것이 목적인 샘이다.   

논문 저자인 lan Goodfellow가 본문에서 든 예시는 지폐위조범과 경찰이다;
![figure2-1](/assets/img/posts/GAN/figure2-1.png)
> 지폐위조범은 경찰을 열심히 속이려고 하고, 경찰은 위조된 지폐를 진짜와 감별하려고 노력한다.
> 이런 경쟁 속에서 두 그룹을 모두 속이고 구별하는 서로의 능력이 발전하게 되면,    
> 결과적으로 진짜 지폐와 위조 지폐를 구별할 수 없는 정도(구별할 확률 $p_d=0.5$)에 이른다. 

### Adversarial Nets (목적함수)
- [Before GAN](https://westshine-data-analysis.tistory.com/83)

D의 입장에서는 data로부터 뽑은 sample x는 D(x)=1이 되고, G에 임의의 noise distribution으로부터 뽑은 input z를 넣고 만든 sample에 대해서는 D(G(z))=0이 되도록 노력한다.    
즉, D는 실수할 확률을 낮추기 위해 노력하고(mini), G는 D가 실수할 확률을 높이기 위해 노력하기(max) 때문에 논문에선 둘을 같이 놓고 **minimax two-player game**이라 표현한다.    

![figure1](/assets/img/posts/GAN/figure1.png)
- 첫째항: real data x를 Discriminator에 넣었을 때 나오는 결과를 log 취해서 얻는 값
- 두번째 항: fake data z를 generator에 넣어서 나온 sample을 Discriminator에 넣어 log 취해서 얻는 값

![figure2-2](/assets/img/posts/GAN/figure2-2.png)

1. D의 입장: D의 판별 성능이 뛰어나다고 보장되었을 때, sample이 실제 데이터라면 D(x)=1이 되어 첫째항 log값은 0으로 사라진다. 두번째 항에서는, 생성된 이미지를 가짜라고 잘 판별할 것이기 때문에, D(G(z))=1이 되어 두번째 항 역시 0이 되며 결과적으로 V(D,G)=0이 된다. 
   - D의 입장에서 얻을수 있는 'max'는 결과적으로 loss 0을 출력한다.
   - 'max': D가 올바르게 라벨을 분류할 확률을 최대화 하는 것
2. G의 입장: G가 D를 속일수 있는 이미지를 생성했을 경우, D(G(z))=1이므로 두번쨰 행은 log0이되어 마이너스 무한대로 간다. 
   - G의 입장에서 얻을 수 있는 'min'은 결과적으로 loss에 대해 -무한대를 출력한다.
   - 'min': log(1-D(G(z))) 즉, D가 생성된 이미지의 라벨을 올바르게 분류 확률을 최소화 시키는 것

> [주의] 
> 학습에서 D를 최적화하는 데에는 많은 계산들을 필요로 하며, 적은 데이터셋에서는 overfitting을 초래하기도 한다.
> 따라서 **k step 만큼 D를 최적화한 후 G는 1 step 만큼만 최적화한다.**
> - 학습 초반에는 G의 성능이 형편없기 때문에, D가 G가 생성한 데이터와 실제 데이터를 너무 잘 구별한다. 
> - 이 경우에, log(1-D(G(z)))는 포화상태가 되기 때문에(즉, gradient가 너무 작은 값이 나와 학습이 느리기 때문에) 
> - **log(1-D(G(z)))를 최소화하기 보다 log(D(G(z)))를 최대화**하게끔 학습하는 것이 좋다.

**GAN의 수렴과정**
![figure2](/assets/img/posts/GAN/figure2.png)
- 검정색: 실제 데이터 distribution(real), 파란색: D distribution, 초록색: G distribution(fake)
위의 figure는 시간에 따른 학습 분포를 나타냈다.
1. a) real과 fake의 분포가 전혀 다르고, D의 성능도 좋지 않음
2. b) D가 real과 fake를 분명하게 판별
3. c) D의 학습이 어느정도 이뤄지면, G는 real 데이터 분포를 따라가며 D가 구별해내기 어려운 데이터를 생성하도록 학습함
4. d) real과 fake의 분포가 거의 비슷해져 구분할 수 없을 만큼 G가 학습하고, D는 결국 둘을 구분할 수 없어 1/2로 수렴한다.

## DCGAN
GAN은 데이터분포의 학습에 있어서 Markov Chain 없이, back-porpagation을 통한 추론으로 배울수 있다는 것이 핵심적이었다.    
그러나 학습에 있어서 D,G의 균형을 잘 맞추기 힘들다는 구조적 불안정함이 단점이었다.

따라서 Convolutional 구조를 GAN에 녹여 구조를 안정화하고자 한것이 DCGAN이다.
DCGAN의 출현으로 아래와 같은 기여를 할수 있었다;
1. 대부분의 상황에서 안정적으로 학습이 가능한 GAN 구조를 찾아낸 점
2. Generator가 벡터산술 연산이 가능하다는 점
3. 특징 filter들이 이미지의 특정 물체를 학습할 수 있었다는 점

### 1. Model Architecture
논문에서는 모델을 어떤식으로 짜야하는지 guide line이 명시되어 있어 그대로 들고 왔다.   
![figure3](/assets/img/posts/GAN/figure3.png)
1. 모든 pooling layer를 D에서는 strided convolution으로, G에서는 fractional-strided convolution으로 대체하라.
2. G와 D 모두에 batch normalization을 추가하라
3. fully connected layer를 모두 제거해라
4. G에서 ReLU activation을 output을 제외한 모든 layer에서 사용하고, output layer는 Tanh를 사용해라
5. D에서는 모든 layer에 Leaky ReLU activation을 사용해라

결과적으로, DCGAN의 Generator 구조는 다음과 같다;   
![figure4](/assets/img/posts/GAN/figure4.png)

### 2. latent Z vector
기존의 word2vec에서 "왕-남자+여자=여왕"과 같은 언어 연산이 가능하다. 이런 연산은 단어의 의미를 이해하고 그에 맞는 새로운 다어를 찾아야하는 고차원 처리가 필요한 문제이다. DCGAN에서는 이러한 연산을 이미지에서도 가능하다는 것을 보였다.    

![figure5](/assets/img/posts/GAN/figure5.png)
z중에 "안경+남자"를 그리게 하는 입력값들을 모아 평균치를 구하고, '안경x 남자"와 "안경x 여자"의 입력값에 대해서도 평균치를 구한 후 각각을 뺴고 더해주면 새로운 z가 출력된다. 새로운 z를 GAN에 넣어 결과 이미지를 받으면 "안경+여자"가 나온다.
input z는 단순 Gaussian noise이기 때문에, 결과 이미지의 품질은 z에 의해서가 아닌 generator의 z를 mapping하는 함수의 역할이다. 따라서 해당 네트워크는 학습데이터를 단순히 암기해서 생성한 것이 아니라는 것이 증명된다. 아래 예시를 보면 더 와닿을 것이다.   
![figure6](/assets/img/posts/GAN/figure6.png)
왼쪽을 보고 있는 얼굴을 만들어내는 input zleft들의 평균 vector z¯left과 오른쪽을 보고 있는 얼굴에 대응하는 zright들의 평균 vector z¯right를 계산하고 이 두 벡터의 사이를 잇는 축(axis)을 interpolating하여 Generator에 넣어보았더니 천천히 "회전(turn)"하는 얼굴들이 나오는 것을 볼 수 있다.

## WGAN(Wasserstein GAN)
여전히 D,G의 균형을 유지하며 학습하는것은 GAN에게 어려운 과제로 남아있으며, 학습이 완료된 이후에도 mode collapsing이 발생한다.
이러한 문제가 발생하는 이유는, D가 선생님 역할을 충분히 해주지 못해 모델이 최적점까지 학습되지 못했기 떄문이다.
> **[Mode dropping]**   
> ![figure2-4](/assets/img/posts/GAN/figure2-4.png)   
> GAN의 학습과정에서 D와 G를 번갈아가며 학습시킨다. 이는 D의 학습과 G의 학습이 서로를 상쇄할 수 있는 문제를 발생시킨다.   
> 즉, D와 G가 서로를 속고 속이며 제자리를 맴돈다면 위의 사진처럼 양쪽 모두 전역해로 수렴할 수 없게된다.   
> 이를 모델이 collapsing한다고 한다.   
> ![figure2-3](/assets/img/posts/GAN/figure2-3.png)   
> 통계학에서 mode는 최빈값, 즉 가장 빈도가 높은 값을 말한다. mode가 여러개 존재하면 어떻게 될까?(multi-mode)    
> mode collapsing은 mode가 여러개인 분포에서 G가 mode 중 하나로만 치우쳐서 분포를 변환시키는 문제를 일컷는다.   
> 말그대로 mode의 충돌을 말하는 것이다. (ex) MNIST를 GAN에 학습시키다보면 같은 숫자만(예를들어 '5'만) 계속해서 생성되는 현상)

WGAN에서는 이러한 문제점을 해결하기 위하여 discriminator를 대신한 **critic**을 사용한다. 
  - critic은 EM distance로부터 얻은 scalar값을 이용한다.
  - EM distance는 확률분포 간 거리를 측정하는 척도 중 하나다.
=> 이 방식으로 mode collapsing을 해결하고, D와 G간의 학습 균형을 계속 살피지 않아도 된다.

### 1. 거리함수
**거리함수란?**   
집합 X 위의 거리 함수는 다음 조건을 만족시키는 함수 $d:X \times X \rightarrow [0,\inf )$이다.
- 임의의 $x,y \in X$에 대해서, $d(x,y)=0 \leftrightarrow x=y$
- 임의의 $x,y \in X$에 대해서, $d(x,y)=d(y,x)$
- 임의의 $x,y \in X$에 대해서, $d(x,y) \le d(x,z)+d(z,y) $

=> 다양한 거리 함수 존재 ex) 유클리드 거리, TV distance, KL Divergence, JS Divergence 등   

**A 확률분포와 B확률분포 간의 차이를 거리함수로 나타낼 수 있을까??**
![figure2-6](/assets/img/posts/GAN/figure2-6.png) 

### 2. Wasserstein-1 Distance (Earth Mover's Distance)
두 확률 분포의 결합확률분포 Π(Pr, Pg)중에서 d(X, Y) (x와 y의 거리)의 기댓값을 가장 작게 추정한 값이다.   
![figure2-7](/assets/img/posts/GAN/figure2-7.png)   
![figure2-8](/assets/img/posts/GAN/figure2-8.png)
- 파란색 원이 X의 분포, 빨간색 원이 Y의 분포, 𝛘가 결합 확률 분포를 의미하며, 초록색 선의 길이가 절댓값 x-y를 의미한다. 
- 즉, 초록색 선 길이들의 기댓값을 가장 작게 추정한 값이다.

문제를 쉽게 다시 정의해보자면, Pr에서 Pg로 mess를 옮길 수 있는 여러 방법 중 변화량의 합이 가장 작은 방법을 채택한다.
![figure2-9](/assets/img/posts/GAN/figure2-9.png)

**EM distance의 유용성**   
다른 거리함수에서는 두 분포가 서로 겹치는 경우에는 0, 겹치지 않는 경우에는 무한대 또는 상수로 극단적인 거리 값을 나타낸다.    
- 이는 discriminator와 generator가 분포를 학습할 때, 초반에는 실제 데이터의 분포와 겹치지 않을 것이므로 무한대 또는 일정한 gradient 상수 값을 갖다가, 갑자기 0으로 변해버리므로 gradient가 제대로 전달되지 않는 문제가 발생하게 된다.   

반면 EM distance의 경우 분포가 겹치던 겹치지 않던 간에 절댓값 θ를 유지할 수 있으므로 학습에 사용하기 쉽다.
 

### 3. WGAN 목적함수
위의 EM distance에서의 inf항을 계산할 수 없다. 따라서 Kantorovich-Rubinstein duality를 이용하여 max 식으로 치환하면 GNA의 loss와 비슷한 모양의 loss function을 얻을 수 있다.
![figure2-11](/assets/img/posts/GAN/figure2-11.png)
- f는 1-Lispschitz 조건을 만족하는 함수로, discriminator(critic) 역할을 하는 함수다
  - 1-Lispschitz: 두 점 사이의 거리를 일정 비 이상으로 증가시키지 않는 함수

![figure2-10](/assets/img/posts/GAN/figure2-10.png) 
![figure2-5](/assets/img/posts/GAN/figure2-5.png) 
- Pr과 p(z) (Pθ역할)를 미니배치만큼 샘플링한 후에, critic의 loss function을 이용하여 parameter w(즉 함수 f)를 update시킨다.
- 여기서 update 후 clip(w, -c, c)라는 부분이 있는데, Lipschitz조건을 만족하도록 parameter w가 [-c, c]공간에 안쪽에 존재하도록 강제하는 것이다 => 이를 Weight clipping이라고 한다.
 
이는 WGAN의 한계점이라고 할 수 있는데, 실험 결과 clipping parameter c 가 크면 limit(c나 -c)까지 도달하는 시간이 오래 걸리기 때문에, optimal 지점까지 학습하는 데 시간이 오래 걸린다. 반면 c가 작으면, gradient vanish 문제가 발생한다

---


#### [Reference]
###### GAN
[paper](https://arxiv.org/pdf/1406.2661.pdf)   
[(GAN)Generative Adversarial Nets 논문 리뷰](https://tobigs.gitbook.io/tobigs/deep-learning/computer-vision/gan-generative-adversarial-network)   
[동빈나 GAN: Generative Adversarial Networks (꼼꼼한 딥러닝 논문 리뷰와 코드 실습)](https://www.youtube.com/watch?v=AVvlDmhHgC4&list=PLRx0vPvlEmdADpce8aoBhNnDaaHQN1Typ&index=10)

###### DCGAN
[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)   
[초짜 대학원생의 입장에서 이해하는 Deep Convolutional Generative Adversarial Network (DCGAN)](https://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html)

###### WGAN
[paper](https://arxiv.org/pdf/1701.07875.pdf)   
[[논문 읽기] Wasserstein GAN](https://ahjeong.tistory.com/7)   
[십분딥러닝_16_WGAN (Wasserstein GANs)](https://www.youtube.com/watch?v=j-6uUCmY4p8)
